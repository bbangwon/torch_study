{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5827c7f6",
   "metadata": {},
   "source": [
    "# MLP Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "726a4791",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a27544cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(z):\n",
    "    z_exp = torch.exp(z)\n",
    "    z_exp_sum = torch.sum(z_exp, dim=1) + 1e-8  # softmax 분모\n",
    "    a = z_exp / z_exp_sum.unsqueeze(dim=1)\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9ab18b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(y_hat, y):\n",
    "    loss = torch.sum(-y * torch.log(y_hat), dim=1)\n",
    "    loss = loss.mean()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee85c84",
   "metadata": {},
   "source": [
    "# MLP Model\n",
    "\n",
    "## Neural Network Design\n",
    "\n",
    "- Pytorch에서 neural network는 torch.nn.Module이라는 베이스 클래스를 상속받아서 사용하고 각 요소들은 torch.nn에서 사용할 수 있음\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7378cbf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.linear1 = nn.Linear(10, 20)\n",
    "        self.linear2 = nn.Linear(20, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.sigmoid(self.linear1(x))\n",
    "        return F.sigmoid(self.linear2(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d502c0",
   "metadata": {},
   "source": [
    "- Linear Layer\n",
    "  - Fully connected layer라고 불름\n",
    "  - torch.nn.Linear(in_features, out_features, bias=True)\n",
    "  - in_features: 입력 perceptron의 수\n",
    "  - out_features: 출력 perceptron의 수\n",
    "  - bias: bias를 사용할 것인지 여부\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e1d616",
   "metadata": {},
   "source": [
    "- Activation function\n",
    "  - Sigmoid\n",
    "    - torch.nn.function.sigmoid\n",
    "    - 0~1\n",
    "  - Tanh\n",
    "    - torch.nn.function.tanh\n",
    "    - -1~1\n",
    "  - ReLU\n",
    "    - torch.nn.function.relu\n",
    "    - max(0, x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a56d096",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPnet(nn.Module):  # nn.Module을 상속받음\n",
    "    def __init__(\n",
    "        self,\n",
    "    ):  # 생성자 정의 (해당 클래스의 인스턴스가 생성될 때 자동으로 호출)\n",
    "        super(MLPnet, self).__init__()  # nn.Module의 생성자(__init__)를 호출\n",
    "        self.fc1 = nn.Linear(\n",
    "            in_features=3 * 32 * 32, out_features=128\n",
    "        )  # 3*32*32 >> 128\n",
    "        self.fc2 = nn.Linear(in_features=128, out_features=64)  # 128 >>> 64\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "        self.fc3 = nn.Linear(in_features=64, out_features=10)  # 64 >>> 10\n",
    "\n",
    "    def forward(self, x):  # forward propagation\n",
    "        x = torch.flatten(x, 1)  # 행렬형태의 입력을 벡터형태로 변형\n",
    "        x = self.fc1(x)  # 1st layer\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)  # 2nd layer\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x)  # 3rd layer\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f9f4413",
   "metadata": {},
   "source": [
    "- Forward Path\n",
    "  - 입력부터 출력까지의 순방향의 과정을 의미\n",
    "  - Forward path를 통해서 최종 Loss를 계산할 수 있음\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c794b8",
   "metadata": {},
   "source": [
    "- Backward Path\n",
    "  - Forward path로부터 게산된 Loss에 대한 각 파라미터 W, b의 gradient 계산해가는 과정(Backpropagation 이라고도 불림)\n",
    "  - 처음부터 계산하는것은 너무 어려움\n",
    "  - 이때 Chain Rule을 이용하면 다음 계산값은 이전 계산값을 활용해서 계산할 수 있음\n",
    "  - Update는 기존 W, b값에서 계산한 gradient의 반대 방향으로 학습률(lerning rate)를 곱하여 변화\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6248f3ba",
   "metadata": {},
   "source": [
    "- torch.optim\n",
    "  - 모델을 업데이트하는 다양한 optimization algorithm을 포함\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ab74828d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4486ae05",
   "metadata": {},
   "source": [
    "- SGD(Stochastic Gradient Descent)\n",
    "  - 확률적 경사 하강법 알고리즘\n",
    "  - 전체 데이터를 전부 고려하지 않고 미니배치에 대한 gradient 기반으로 업데이트 함\n",
    "  - 미니 배치를 선택하는 것이 확률적이기 때문에 Stochastic이라는 표현이 붙음\n",
    "  - Pytorch에서는 Full-batch Gradient Descent(FGD)를 따로 제공하지 않고 사용자가 미니배치 사이즈를 전체 데이터로 지정하면 FGD로 사용할 수 있음\n",
    "  - Optimizer에는 Model의 parameters의 정보를 입력으로 넣어주어야 함\n",
    "  - 학습률(learning rate)을 지정하고, 그 외에 momentum, weight_decay, Nesterov등의 옵션을 고려\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a6faa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLPnet()\n",
    "optimizer = optim.SGD(\n",
    "    model.parameters(), lr=0.01, momentum=0.9, weight_decay=0.0005, nesterov=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "955b3439",
   "metadata": {},
   "source": [
    "- weight decay\n",
    "  - 모든 레이어의 W 파라미터에 대해서 적용되며 W 값이 과도하게 커지지 않도록 W에 L2-norm을 적용시킨 크기 값을 Loss식에 추가한 것\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a47a0517",
   "metadata": {},
   "source": [
    "# Adam(Adaptive Moment Estimation)\n",
    "\n",
    "- momentum의 방향과 크기를 모두 고려한 Optimization 알고리즘\n",
    "- SGD와 마찬가지고 lr을 설정할 수 있고, betas라는 parameters가 있으며 기본 값인(0.9, 0.999)를 사용하는 것을 권장\n",
    "- Gradient에 따라서 적응적으로 업데이트 값을 적용하는 효과가 있어서 이론적으로는 learning rate값에 민감하지 않고 큰 값을 적용하는 것도 가능\n",
    "- Adam이 SGD보다 더 나중에 고안된 optimizer이지만 항상 Adam이 더 좋은 성능을 내는 것이 아님(Dataset이나 Task에 따라 다름)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e1b426",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=0.001, betas=(0.9, 0.999))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cada12c9",
   "metadata": {},
   "source": [
    "# Scheduler\n",
    "\n",
    "- 학습의 진행정도에 따라 learning rate를 조절하는 역할\n",
    "- StepLR:\n",
    "  - torch.optim.lr_scheduler.StepLR(optimizer, step_size=40, gamma=0.1)\n",
    "  - step_size에 도달하면 learning rate에 gamma를 곱함\n",
    "- MultiStepLR\n",
    "  - torch.optim.lr_schduler.MultiStepLR(optimizer, milestones=[30,80], gamma=0.1)\n",
    "  - 복수의 step_size를 milestones으로 정의하여 각 milestones에 도달하면 learning rate에 gamma를 곱함\n",
    "- CosineAnnealingLR:\n",
    "  - torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=20)\n",
    "  - Cosine 개형을 따라가며 learning rate가 정해짐\n",
    "  - T_max: cosine 주기의 1/2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba31414",
   "metadata": {},
   "source": [
    "- StepLR의 경우 Step size를 어떻게 정하는가?\n",
    "  - Loss가 줄어들지 않는 포인트에서 일반적으로 Learning rate를 낮춰줌\n",
    "  - 주의점은 Loss가 작은 것이 항상 좋은 것은 아님\n",
    "  - Trainig Loss는 작지만 Test에서 성능이 좋지 못할 수도 있음(overfitting)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1ca0d89c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets\n",
    "\n",
    "\n",
    "# 데이터 불러오기: torchvision 패키지에서 cifar10의 train data 와 test data를 불러옴\n",
    "def load_data(dataset, dataset_path):\n",
    "    train_dataset = None\n",
    "    test_dataset = None\n",
    "\n",
    "    if dataset == \"CIFAR10\":\n",
    "        train_dataset = datasets.CIFAR10(\n",
    "            dataset_path,  # 해당 데이터 셋의 path\n",
    "            download=True,  # 해당 데이터셋을 다운 받을 것인지\n",
    "            train=True,\n",
    "        )  # 학습 용도로 사용될 것인지\n",
    "        train_dataset.data = torch.tensor(\n",
    "            train_dataset.data\n",
    "        )  # numpy array 형태를 torch tensor로 변경\n",
    "        train_dataset.data = torch.permute(\n",
    "            train_dataset.data, dims=(0, 3, 1, 2)\n",
    "        )  # (B,H,W,C) -> (B,C,H,W) 형태로 변경\n",
    "        train_dataset.targets = torch.tensor(train_dataset.targets)\n",
    "\n",
    "        test_dataset = datasets.CIFAR10(dataset_path, download=False, train=False)\n",
    "        test_dataset.data = torch.tensor(test_dataset.data)\n",
    "        test_dataset.data = torch.permute(test_dataset.data, dims=(0, 3, 1, 2))\n",
    "        test_dataset.targets = torch.tensor(test_dataset.targets)\n",
    "\n",
    "    else:\n",
    "        print(\"Incorrect dataset!\")\n",
    "    return train_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c72d032",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input Preprocess: Model이 학습할 수 있도록 input으로 들어가는 train data를 수정함\n",
    "def input_preprocess(x, train_mode=True):\n",
    "    x = x / 255.0  # MinmaxScaler : input data 를 0~1사이의 float로 바꿔주기\n",
    "    if train_mode:\n",
    "        half = int(x.shape[0] / 2)  # 미니배치 개수의 반\n",
    "        x[0:half, :] = torch.flip(x[0:half, :], dims=[3])  # (B,C,H,W)의 W를 반전\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a7a5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def onehot_encoding(y, n_class):\n",
    "    out = torch.zeros(\n",
    "        len(y), n_class\n",
    "    )  # 전체 클래스 수 만큼 원소를 갖는 zero vector 들을 생성\n",
    "    for i in range(len(y)):  # 미니배치 수(y의 길이) 만큼 반복\n",
    "        out[i, y[i]] = 1  # target index를 1로 채워두기\n",
    "    y = out.float()  # 추후 연산을 위해서 dtype을 float으로 변환\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "948051c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, scheduler, train_iter, train_samp, train_dataset):\n",
    "    model.train(True)  # training mode on (True : train, False : test)\n",
    "    class_number = len(train_dataset.classes)\n",
    "\n",
    "    # Data를 load하는 부분에서 input data와 label 각각에 .to(device)를 적용\n",
    "    # device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    for i in range(train_iter):\n",
    "        batch_index = train_samp.get_random_idx()  # mini-batch만큼 index 가져오기\n",
    "        x, label = train_dataset.data[batch_index], train_dataset.targets[batch_index]\n",
    "\n",
    "        # x = x.to(device)\n",
    "        # label = label.to(device)\n",
    "\n",
    "        x = input_preprocess(x)  # data preprocessing\n",
    "        label = onehot_encoding(label, class_number)  # one-hot encoding\n",
    "        logits = model(x)  # forward propagation (model.forward(x)와 동일)\n",
    "        inference = softmax(logits)  # 전체의 합이 1이 되도록 확률값으로 변환(softmax)\n",
    "        loss = cross_entropy(inference, label)  # Loss 계산\n",
    "        optimizer.zero_grad()  # gradient 0으로 초기화. 초기화 하지 않으면 쌓일수 있음\n",
    "        loss.backward()  # back propagation (그래디언트 계산 -> 어느 방향으로 업데이트 하면 될지 계산됨)\n",
    "        optimizer.step()  # update parameters(여기서 업데이트를 함)\n",
    "        scheduler.step()  # scheduler의 step+1\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e987c4",
   "metadata": {},
   "source": [
    "- model.train(True)로 모델의 학습/비학습 상태임을 정의\n",
    "- 클래스의 수를 변수로 지정\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "477c6561",
   "metadata": {},
   "source": [
    "## Test함수 구현\n",
    "\n",
    "- Test는 train과 달리 랜덤하게 data를 추출할 필요 없음\n",
    "- 미니배치 size로 나누어 남는 data에 대해서도 반드시 forward path를 진행(Computing power가 충분하면 full-batch로 한번에 계산해도 무방)\n",
    "- softmax를 통과하고 나온 예측 값 중 확률이 가장 높은 index를 추측 값으로 예측\n",
    "  - ex)[0.1,0.6,0.09, 0.04, 0.05, 0.01, 0.04, 0.05, 0.01, 0.01] -> 두번째 class로 예측\n",
    "- Accuracy = (맞춘 개수) / (전체 개수)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef7c49d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, test_dataset, b_size):\n",
    "    model.train(False)  # training mode off\n",
    "    test_acc = 0.0\n",
    "    test_iterations = len(test_dataset) // b_size  # 총 iterations 수 계산\n",
    "    for i in range(test_iterations + 1):  # 나머지 한 번에 대해서 추가\n",
    "        test_x = test_dataset.data[i * b_size : i * b_size + b_size]\n",
    "        test_label = test_dataset.targets[i * b_size : i * b_size + b_size]\n",
    "        test_x = input_preprocess(test_x, False)\n",
    "        test_output = model(test_x)  # forward path\n",
    "        _, preds = torch.max(test_output, 1)  # softmax를 생략하고 최대값의 index 찾음\n",
    "        test_acc += torch.sum(\n",
    "            preds == test_label.data\n",
    "        )  # mini-batch 안에서 맞춘 개수를 더해줌\n",
    "\n",
    "    print(\n",
    "        f\"Test Accuracy: {test_acc / len(test_dataset) * 100:.2f}%\"\n",
    "    )  # 최종 성능을 백분율로 출력"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41553eb5",
   "metadata": {},
   "source": [
    "- model.train(False) 로 모델의 비학습 상태임을 정의\n",
    "- test_acc를 0으로 설정하여 맞춘 개수를 카운트\n",
    "- test 데이터를 미니배치로 나눈 나머지에 대해서도 추론 값을 평가해야 하므로 test_iterations에 1을 더한 값으로 for loop 수행\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d6adfee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import RandomSampler\n",
    "\n",
    "model = MLPnet()\n",
    "# Model을 to(device)를 통해서 cuda가 available한 경우 적용\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model.to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=1000, gamma=0.5)\n",
    "\n",
    "dataset_path = \"./cifar10_data/\"\n",
    "dataset_name = \"CIFAR10\"\n",
    "train_dataset, test_dataset = load_data(dataset_name, dataset_path)\n",
    "\n",
    "train_batch_size = 64\n",
    "test_batch_size = 64\n",
    "\n",
    "# 배치사이즈가 크면 클수록 epoch안에 도는 iteration 수가 줄어듬\n",
    "# 배치사이즈가 작게 되면 많은 iteration을 돌려야지 비로소 한 번에 epoch을 돌 수 있음\n",
    "train_iter = 3000\n",
    "train_samp = RandomSampler(len(train_dataset), train_batch_size)\n",
    "\n",
    "model = train(model, optimizer, scheduler, train_iter, train_samp, train_dataset)\n",
    "test(model, test_dataset, test_batch_size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_study",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
