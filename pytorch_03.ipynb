{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5827c7f6",
   "metadata": {},
   "source": [
    "# MLP Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "726a4791",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a27544cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(z):\n",
    "    z_exp = torch.exp(z)\n",
    "    z_exp_sum = torch.sum(z_exp, dim=1) + 1e-8  # softmax 분모\n",
    "    a = z_exp / z_exp_sum.unsqueeze(dim=1)\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9ab18b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(y_hat, y):\n",
    "    loss = torch.sum(-y * torch.log(y_hat), dim=1)\n",
    "    loss = loss.mean()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee85c84",
   "metadata": {},
   "source": [
    "# MLP Model\n",
    "\n",
    "## Neural Network Design\n",
    "\n",
    "- Pytorch에서 neural network는 torch.nn.Module이라는 베이스 클래스를 상속받아서 사용하고 각 요소들은 torch.nn에서 사용할 수 있음\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7378cbf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.linear1 = nn.Linear(10, 20)\n",
    "        self.linear2 = nn.Linear(20, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.sigmoid(self.linear1(x))\n",
    "        return F.sigmoid(self.linear2(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d502c0",
   "metadata": {},
   "source": [
    "- Linear Layer\n",
    "  - Fully connected layer라고 불름\n",
    "  - torch.nn.Linear(in_features, out_features, bias=True)\n",
    "  - in_features: 입력 perceptron의 수\n",
    "  - out_features: 출력 perceptron의 수\n",
    "  - bias: bias를 사용할 것인지 여부\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e1d616",
   "metadata": {},
   "source": [
    "- Activation function\n",
    "  - Sigmoid\n",
    "    - torch.nn.function.sigmoid\n",
    "    - 0~1\n",
    "  - Tanh\n",
    "    - torch.nn.function.tanh\n",
    "    - -1~1\n",
    "  - ReLU\n",
    "    - torch.nn.function.relu\n",
    "    - max(0, x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a56d096",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPnet(nn.Module):  # nn.Module을 상속받음\n",
    "    def __init__(\n",
    "        self,\n",
    "    ):  # 생성자 정의 (해당 클래스의 인스턴스가 생성될 때 자동으로 호출)\n",
    "        super(MLPnet, self).__init__()  # nn.Module의 생성자(__init__)를 호출\n",
    "        self.fc1 = nn.Linear(\n",
    "            in_features=3 * 32 * 32, out_features=128\n",
    "        )  # 3*32*32 >> 128\n",
    "        self.fc2 = nn.Linear(in_features=128, out_features=64)  # 128 >>> 64\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "        self.fc3 = nn.Linear(in_features=64, out_features=10)  # 64 >>> 10\n",
    "\n",
    "    def forward(self, x):  # forward propagation\n",
    "        x = torch.flatten(x, 1)  # 행렬형태의 입력을 벡터형태로 변형\n",
    "        x = self.fc1(x)  # 1st layer\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)  # 2nd layer\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x)  # 3rd layer\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f9f4413",
   "metadata": {},
   "source": [
    "- Forward Path\n",
    "  - 입력부터 출력까지의 순방향의 과정을 의미\n",
    "  - Forward path를 통해서 최종 Loss를 계산할 수 있음\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c794b8",
   "metadata": {},
   "source": [
    "- Backward Path\n",
    "  - Forward path로부터 게산된 Loss에 대한 각 파라미터 W, b의 gradient 계산해가는 과정(Backpropagation 이라고도 불림)\n",
    "  - 처음부터 계산하는것은 너무 어려움\n",
    "  - 이때 Chain Rule을 이용하면 다음 계산값은 이전 계산값을 활용해서 계산할 수 있음\n",
    "  - Update는 기존 W, b값에서 계산한 gradient의 반대 방향으로 학습률(lerning rate)를 곱하여 변화\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6248f3ba",
   "metadata": {},
   "source": [
    "- torch.optim\n",
    "  - 모델을 업데이트하는 다양한 optimization algorithm을 포함\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ab74828d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4486ae05",
   "metadata": {},
   "source": [
    "- SGD(Stochastic Gradient Descent)\n",
    "  - 확률적 경사 하강법 알고리즘\n",
    "  - 전체 데이터를 전부 고려하지 않고 미니배치에 대한 gradient 기반으로 업데이트 함\n",
    "  - 미니 배치를 선택하는 것이 확률적이기 때문에 Stochastic이라는 표현이 붙음\n",
    "  - Pytorch에서는 Full-batch Gradient Descent(FGD)를 따로 제공하지 않고 사용자가 미니배치 사이즈를 전체 데이터로 지정하면 FGD로 사용할 수 있음\n",
    "  - Optimizer에는 Model의 parameters의 정보를 입력으로 넣어주어야 함\n",
    "  - 학습률(learning rate)을 지정하고, 그 외에 momentum, weight_decay, Nesterov등의 옵션을 고려\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a6faa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLPnet()\n",
    "optimizer = optim.SGD(\n",
    "    model.parameters(), lr=0.01, momentum=0.9, weight_decay=0.0005, nesterov=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "955b3439",
   "metadata": {},
   "source": [
    "- weight decay\n",
    "  - 모든 레이어의 W 파라미터에 대해서 적용되며 W 값이 과도하게 커지지 않도록 W에 L2-norm을 적용시킨 크기 값을 Loss식에 추가한 것\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a47a0517",
   "metadata": {},
   "source": [
    "# Adam(Adaptive Moment Estimation)\n",
    "\n",
    "- momentum의 방향과 크기를 모두 고려한 Optimization 알고리즘\n",
    "- SGD와 마찬가지고 lr을 설정할 수 있고, betas라는 parameters가 있으며 기본 값인(0.9, 0.999)를 사용하는 것을 권장\n",
    "- Gradient에 따라서 적응적으로 업데이트 값을 적용하는 효과가 있어서 이론적으로는 learning rate값에 민감하지 않고 큰 값을 적용하는 것도 가능\n",
    "- Adam이 SGD보다 더 나중에 고안된 optimizer이지만 항상 Adam이 더 좋은 성능을 내는 것이 아님(Dataset이나 Task에 따라 다름)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e1b426",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=0.001, betas=(0.9, 0.999))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cada12c9",
   "metadata": {},
   "source": [
    "# Scheduler\n",
    "\n",
    "- 학습의 진행정도에 따라 learning rate를 조절하는 역할\n",
    "- StepLR:\n",
    "  - torch.optim.lr_scheduler.StepLR(optimizer, step_size=40, gamma=0.1)\n",
    "  - step_size에 도달하면 learning rate에 gamma를 곱함\n",
    "- MultiStepLR\n",
    "  - torch.optim.lr_schduler.MultiStepLR(optimizer, milestones=[30,80], gamma=0.1)\n",
    "  - 복수의 step_size를 milestones으로 정의하여 각 milestones에 도달하면 learning rate에 gamma를 곱함\n",
    "- CosineAnnealingLR:\n",
    "  - torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=20)\n",
    "  - Cosine 개형을 따라가며 learning rate가 정해짐\n",
    "  - T_max: cosine 주기의 1/2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba31414",
   "metadata": {},
   "source": [
    "- StepLR의 경우 Step size를 어떻게 정하는가?\n",
    "  - Loss가 줄어들지 않는 포인트에서 일반적으로 Learning rate를 낮춰줌\n",
    "  - 주의점은 Loss가 작은 것이 항상 좋은 것은 아님\n",
    "  - Trainig Loss는 작지만 Test에서 성능이 좋지 못할 수도 있음(overfitting)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1ca0d89c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets\n",
    "\n",
    "\n",
    "# 데이터 불러오기: torchvision 패키지에서 cifar10의 train data 와 test data를 불러옴\n",
    "def load_data(dataset, dataset_path):\n",
    "    train_dataset = None\n",
    "    test_dataset = None\n",
    "\n",
    "    if dataset == \"CIFAR10\":\n",
    "        train_dataset = datasets.CIFAR10(\n",
    "            dataset_path,  # 해당 데이터 셋의 path\n",
    "            download=True,  # 해당 데이터셋을 다운 받을 것인지\n",
    "            train=True,\n",
    "        )  # 학습 용도로 사용될 것인지\n",
    "        train_dataset.data = torch.tensor(\n",
    "            train_dataset.data\n",
    "        )  # numpy array 형태를 torch tensor로 변경\n",
    "        train_dataset.data = torch.permute(\n",
    "            train_dataset.data, dims=(0, 3, 1, 2)\n",
    "        )  # (B,H,W,C) -> (B,C,H,W) 형태로 변경\n",
    "        train_dataset.targets = torch.tensor(train_dataset.targets)\n",
    "\n",
    "        test_dataset = datasets.CIFAR10(dataset_path, download=False, train=False)\n",
    "        test_dataset.data = torch.tensor(test_dataset.data)\n",
    "        test_dataset.data = torch.permute(test_dataset.data, dims=(0, 3, 1, 2))\n",
    "        test_dataset.targets = torch.tensor(test_dataset.targets)\n",
    "\n",
    "    else:\n",
    "        print(\"Incorrect dataset!\")\n",
    "    return train_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d6adfee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import RandomSampler\n",
    "\n",
    "model = MLPnet()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=1000, gamma=0.5)\n",
    "\n",
    "dataset_path = \"./cifar10_data/\"\n",
    "dataset_name = \"CIFAR10\"\n",
    "train_dataset, test_dataset = load_data(dataset_name, dataset_path)\n",
    "\n",
    "train_batch_size = 64\n",
    "test_batch_size = 64\n",
    "train_iter = 3000\n",
    "train_samp = RandomSampler(len(train_dataset), train_batch_size)\n",
    "\n",
    "model = train(model, optimizer, scheduler, train_iter, train_samp, train_dataset)\n",
    "test(model, test_dataset, test_batch_size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_study",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
